"""
Q-learning æ™ºèƒ½ä½“ Â· ä¸ demo å…±ç”¨ igt_envï¼Œå¯å‰ç«¯è¿è¡Œæˆ–è‡ªåŠ¨åŒ–è¿è¡Œã€‚
- å‰ç«¯: streamlit run run_qlearning.py
- è‡ªåŠ¨åŒ–: python run_qlearning.py --auto   ï¼ˆåœ¨ç»ˆç«¯è‡ªåŠ¨è·‘å®Œï¼Œæ‰“å°æ—¥å¿—ä¸ç»“æœï¼‰
"""

import sys
import time
import numpy as np
from igt_env import IGTEnv


def run_qlearning_one_step(env, Q, alpha, epsilon, gamma):
    """æ‰§è¡Œä¸€æ­¥ Q-learningï¼Œè¿”å› (choice, reward, new_Q)ã€‚"""
    decks = list(IGTEnv.DECK_NAMES)
    if np.random.random() < epsilon:
        choice = np.random.choice(decks)
    else:
        choice = max(decks, key=lambda a: Q[a])
    r = env.step(choice)
    Q = dict(Q)
    max_next_q = max(Q.values())
    Q[choice] += alpha * (r + gamma * max_next_q - Q[choice])
    return choice, r, Q


def run_qlearning_auto(env, n_trials, alpha=0.15, epsilon=0.1, gamma=0.0, seed=42):
    """æ—  UIï¼šç”¨ä¸ demo ç›¸åŒçš„ igt_env è‡ªåŠ¨è·‘å®Œï¼Œè¿”å› (path_rows, balances)ã€‚"""
    if seed is not None:
        np.random.seed(seed)
    env.reset(initial_balance=2000)
    decks = list(IGTEnv.DECK_NAMES)
    Q = {a: 0.0 for a in decks}
    path_rows = []
    balances = [2000]
    for t in range(1, n_trials + 1):
        if np.random.random() < epsilon:
            choice = np.random.choice(decks)
        else:
            choice = max(decks, key=lambda a: Q[a])
        r = env.step(choice)
        max_next_q = max(Q.values())
        Q[choice] += alpha * (r + gamma * max_next_q - Q[choice])
        path_rows.append((t, choice, r, env.balance))
        balances.append(env.balance)
    return path_rows, balances


def run_qlearning_step_by_step(env, n_trials, alpha, epsilon, gamma, seed, step_delay, log_ph, balance_ph, path_ph, prop_ph, chart_ph):
    """é€æ­¥æ‰§è¡Œ Q-learningï¼Œå¹¶æ›´æ–°å‰ç«¯å ä½ç¬¦ã€‚å¸ƒå±€ï¼šå·¦ä¸Šæ˜¯æ—¥å¿—ã€å·¦ä¸‹æ˜¯å†å²è¡¨ï¼›å³ä¸Šæ˜¯ç‰Œå †æ¯”ä¾‹ã€å³ä¸‹æ˜¯æ”¶ç›Šæ›²çº¿ã€‚"""
    if seed is not None:
        np.random.seed(seed)
    env.reset(initial_balance=2000)
    decks = list(IGTEnv.DECK_NAMES)
    Q = {a: 0.0 for a in decks}

    log_lines = []
    path_rows = []
    balances = [2000]

    for t in range(1, n_trials + 1):
        if np.random.random() < epsilon:
            choice = np.random.choice(decks)
            mode = "æ¢ç´¢"
        else:
            choice = max(decks, key=lambda a: Q[a])
            mode = "åˆ©ç”¨"
        r = env.step(choice)
        max_next_q = max(Q.values())
        Q[choice] += alpha * (r + gamma * max_next_q - Q[choice])

        path_rows.append((t, choice, r, env.balance))
        balances.append(env.balance)
        line = f"ç¬¬ {t:4d} è½®  [{mode}] é€‰æ‹© ç‰Œå † {choice}  â†’  æ”¶ç›Š {r:+5d}  â†’  ä½™é¢ {env.balance:6d}  |  Q = {dict((k, round(v, 1)) for k, v in Q.items())}"
        log_lines.append(line)

        if log_ph is not None:
            log_ph.text_area(
                "è¿è¡Œæ—¥å¿—",
                value="\n".join(log_lines),
                height=400,
                disabled=True,
                key=f"ql_log_{t}",
                label_visibility="collapsed",
            )
        if balance_ph is not None:
            balance_ph.metric("å½“å‰ä½™é¢", f"Â¥ {env.balance}", delta=f"{r:+d}")
        if path_ph is not None:
            path_ph.dataframe(
                data=[{"è½®æ¬¡": r[0], "é€‰æ‹©": r[1], "æ”¶ç›Š": r[2], "ä½™é¢": r[3]} for r in path_rows[-30:]],
                use_container_width=True,
                height=220,
                hide_index=True,
            )
        if prop_ph is not None and path_rows:
            from collections import Counter
            counts = Counter(r[1] for r in path_rows)
            prop_ph.bar_chart({d: [counts.get(d, 0)] for d in decks}, height=220)
        if chart_ph is not None and len(balances) > 1:
            chart_ph.line_chart({"ä½™é¢": balances}, height=220)

        if step_delay > 0:
            time.sleep(step_delay)

    return path_rows, balances, log_lines


def _main_auto():
    """è‡ªåŠ¨åŒ–è¿è¡Œï¼šç”¨ä¸ demo ç›¸åŒçš„ igt_env åœ¨ç»ˆç«¯è·‘å®Œå¹¶æ‰“å°ã€‚"""
    import argparse
    p = argparse.ArgumentParser(description="Q-learning Â· è‡ªåŠ¨åŒ–è·‘ IGTï¼ˆä¸ demo åŒä¸€ç¯å¢ƒï¼‰")
    p.add_argument("--auto", "-a", action="store_true", help="è‡ªåŠ¨è·‘å®Œï¼Œä¸å¯åŠ¨ Streamlit")
    p.add_argument("--trials", "-n", type=int, default=200, help="è¯•éªŒè½®æ•°")
    p.add_argument("--alpha", type=float, default=0.15, help="å­¦ä¹ ç‡ Î±")
    p.add_argument("--epsilon", type=float, default=0.1, help="æ¢ç´¢ç‡ Îµ")
    p.add_argument("--gamma", type=float, default=0.0, help="æŠ˜æ‰£å› å­ Î³")
    p.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    p.add_argument("--log-every", type=int, default=50, help="æ¯ N è½®æ‰“å°ä¸€è¡Œ")
    args = p.parse_args()
    env = IGTEnv(seed=args.seed)
    path_rows, balances = run_qlearning_auto(env, args.trials, args.alpha, args.epsilon, args.gamma, args.seed)
    for i in range(0, len(path_rows), args.log_every):
        t, choice, r, bal = path_rows[i]
        print(f"ç¬¬ {t:4d} è½®  é€‰æ‹© ç‰Œå † {choice}  â†’  æ”¶ç›Š {r:+5d}  â†’  ä½™é¢ {bal:6d}")
    print("---")
    print(f"è¿è¡Œç»“æŸ Â· å…± {args.trials} è½® Â· æœ€ç»ˆä½™é¢ Â¥ {env.balance}")


if __name__ == "__main__":
    if "--auto" in sys.argv or "-a" in sys.argv:
        _main_auto()
        sys.exit(0)

    import streamlit as st
    st.set_page_config(page_title="Q-learning Â· IGT", page_icon="ğŸ¤–", layout="wide")
    if "igt_decks" not in st.session_state:
        st.session_state.igt_decks = {k: list(v) for k, v in IGTEnv.DEFAULT_DECKS.items()}

    st.title("ğŸ¤– Q-learning æ™ºèƒ½ä½“ Â· çˆ±è·åèµŒåšä»»åŠ¡")
    st.caption("åœ¨å‰ç«¯é€æ­¥è§‚çœ‹ç®—æ³•é€‰æ‹©è·¯å¾„ä¸è¿è¡Œæ—¥å¿—ï¼Œä¸äººç©æ—¶çœ‹åˆ°çš„è·¯å¾„ä¸€è‡´ã€‚")
    st.markdown("---")

    with st.sidebar:
        st.subheader("å‚æ•°")
        n_trials = st.number_input("è¯•éªŒè½®æ•°", min_value=10, max_value=2000, value=200, step=10)
        alpha = st.slider("å­¦ä¹ ç‡ Î±", 0.01, 0.5, 0.15, 0.01)
        gamma = st.slider("æŠ˜æ‰£å› å­ Î³ (è¿œè§åŠ›)", 0.0, 0.99, 0.0, 0.01, help="è¡¡é‡æœªæ¥å¥–åŠ±å¯¹å½“å‰å†³ç­–çš„å½±å“")
        epsilon = st.slider("æ¢ç´¢ç‡ Îµ", 0.0, 0.5, 0.1, 0.01)
        seed = st.number_input("éšæœºç§å­ï¼ˆå¯é€‰ï¼‰", min_value=0, value=42, step=1)
        step_delay = st.slider("æ¯æ­¥å»¶è¿Ÿï¼ˆç§’ï¼‰", 0.0, 0.5, 0.05, 0.01, help="0 = æœ€å¿«ï¼Œç¨å¤§ä¸€ç‚¹å¯çœ‹æ¸…æ¯ä¸€æ­¥")
        st.markdown("---")
        run_clicked = st.button("â–¶ å¼€å§‹è¿è¡Œ Q-learning", type="primary")

    if run_clicked:
        env = IGTEnv(seed=seed, decks=st.session_state.igt_decks)
        balance_ph = st.empty()
        col_left, col_right = st.columns([1, 1])
        with col_left:
            log_ph = st.empty()
            path_ph = st.empty()
        with col_right:
            prop_ph = st.empty()
            chart_ph = st.empty()

        with st.spinner("Q-learning è¿è¡Œä¸­..."):
            path_rows, balances, _ = run_qlearning_step_by_step(
                env, n_trials, alpha, epsilon, gamma, seed, step_delay,
                log_ph, balance_ph, path_ph, prop_ph, chart_ph,
            )

        st.success(f"è¿è¡Œç»“æŸ Â· æœ€ç»ˆä½™é¢ Â¥ {env.balance} Â· å…± {n_trials} è½®")
        st.balloons()
    else:
        st.info("ğŸ‘ˆ åœ¨å·¦ä¾§è®¾ç½®å‚æ•°åç‚¹å‡»ã€Œå¼€å§‹è¿è¡Œ Q-learningã€ï¼Œå³å¯åœ¨å‰ç«¯é€æ­¥çœ‹åˆ°é€‰æ‹©è·¯å¾„ä¸æ—¥å¿—ã€‚")
